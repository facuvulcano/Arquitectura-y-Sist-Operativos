{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "# **Procesos**\n",
    "\n",
    "## **Que es un proceso?**\n",
    "\n",
    "Un proceso es una abstraccion que tiene el SO sobre una tarea que se esta ejecutando. Contiene informacion del programa como, el codigo, datos que se generaron al programa, datos que se generaron al ejecutar, recursos virtuales, informacion de prioridad de ejecucion, informacion de seguridad y el estado de los registros. Toda esta informacion esta alamcenada en una tabla de procesos. Cada proceso tiene su propia tabla de procesos con un PID (indice generado al crear el proceso) que lo identificara.\n",
    "\n",
    "## **Que es el multiprocesamiento?**\n",
    "\n",
    "El multiprocesamiento se refiere a la utilizacion de multiples unidades de procesamiento dentro de una misma computadora para ejecutar tareas simultaneas. Esta capacidad permite mejorar el rendimiento, la eficiencia y la capacidad de respuesta del sistema.\n",
    "\n",
    "## **En un sistema con un solo procesador como se hace para ejecutar multiples procesos?**\n",
    "\n",
    "Cuando tenemos un solo procesador para lograr la ejecucion de multiples procesos al mismo tiempo el procesdor asigna un intervalo de tiempo (time slice) a cada proceso, de esta forma se lograra concurrencia virtual que es que aunque solo un proceso se ejecuta fisicamente en un momento dado, la alternancia rapida crea la ilusion de ejecucion simultanea. \n",
    "\n",
    "## **En un sistema con multiples procesadores como se hace para ejecutar multiples procesos?**\n",
    "\n",
    "Aqui tenemos paralelismo real, varios procesos pueden ejecutarse verdaderamente al mismo tiempo en diferentes procesadores. Hay una distribucion de cargas, el SO puede distribuir los procesos de manera equilibrada entre los procesadores disponibles.\n",
    "\n",
    "## **Como se gestionan los procesos en un sistema multiprocesador**\n",
    "\n",
    "A traves de la planificacion de procesos (scheduling) el SO debe decidir que procesos se ejecutan en que procesadores y cuando. Ademas hay que lograr la sincronizacion y comunicacion entre procesos, cuando multiples procesos se ejecutan en diferentes procesadores, es esencial gestionar el acceso a recursos compartidos para evitar condiciones de carrera y garantizar la coherencia de los datos.\n",
    "\n",
    "## **Cuales son las ventajas del multiprocesamiento?**\n",
    "\n",
    "Algunas de las ventajas del multiprocesamiento son; Mayor rendimiento: capacidad de ejecutar multiples procesos simultaneamente reduciendo el tiempo total de ejecucion. Mejor utilizacion de recursos: Los procesadores adicionales pueden manejar tareas de E/S mientras otros procesadores ejecutan calculos intensivos. Escalabilidad: Facil incorporacion de mas procesadores para aumentar la capacaidad de procesamiento segun sea necesario. Fiabilidad y tolerancia a fallos, paralelismo.\n",
    "\n",
    "## **Cuales son las desventajas del multiprocesamiento**\n",
    "\n",
    "Complejidad en la gestion de SO: requiere mecanismos avanzados para la planificacion, sincronizacion y comunicacion entre procesos. Coherencia de datos: mantener la coherencia de la memoria compartida y las caches es tecnicamente complejo. Problemas de sincronizacion: evitar condiciones de carrera y garatnizar el acceso seguro a recursos compartidos puede ser dificil. Costo y consumo de energia: Sistemas con multiples procesadores son mas costosos y consumen mas energia.\n",
    "\n",
    "## **Que es el tiempo virtual?**\n",
    "\n",
    "En un sistema multiprocesador el tiempo virtuaul se refiere a la abstraccion del tiempo real que perciben los procesos en ejecucion. Aunque multiples procesadores pueden ejecutar procesos simultaneamente, la gestion del tiempo y la asignacion de recursos aun juega un papel crucial en como los procesos perciben su ejecucion. El tiempo virtual permite la simulacion de paralelismo y el aislamiento de procesos. \n",
    "\n",
    "## **Como se crea un proceso?**\n",
    "\n",
    "La creacion de un proceso es el primer paso en el ciclo de vida de un proceso. Un proceso nace cuando se ejecuta una tarea, ya sea iniciada automaticamente al arrancar el sistema (como el shell) o por una solicitud explicita de otro proceso.\n",
    "\n",
    "## **Como se crea un proceso en windows?**\n",
    "\n",
    "En windows, la creacion de procesos se maneja principalmente a trave de la API de windows. Las funciones claves involucradas son CreateProcessAsUser() y SetPriorityClass(). CreateProcessAsUser() es una funcion que permite a un proceso crear un nuevo proceso que se ejecuta en el contexto de un usuario especifico. Es especialmente util en escenarios donde se necesita lanzar procesos con privilegios distintos o en nombre de otro usuario. SetPriorityClass() permite ajustar la prioridad de ejecucion de un proceso, lo que influye en la cantidad de tiempo de CPU que recibe en comparacion con otros procesos. Al establecer una clase de prioridad mas alta, el proceso tendra mayor acceso al tiempo de CPU, mejorando su rendimiento en comparacion con procesos de prioridad mas baja. Flujo de creacion: inicio del proceso, llamada a CreateProcessAsUser(), asignacion de recursos, ejecucion y ajustes adicionales (SetPriorityClass()).\n",
    "\n",
    "## **Como se crea un proceso en Linux?**\n",
    "\n",
    "En Linux, la creacion de procesos se basa en dos funciones principales: fork() y execve(). fork() es una llamada al sistema que crea un nuevo proceso duplicando el proceso existente (proceso padre). El nuevo proceso creado se conoce como proceso hijo. El proceso padre recibe el PID del hijo recien creado y el procreso hijo recibe 0 como valor de retorno de fork(). Por otro lado execve() reemplaza el espacio de direcciones del proceso actual con un nuevo programa. Es comunmente utilizado despues del fork() para que el proceso hijo ejecute un prgrama diferente al padre. Proceso de creacion completo en linux: inicio del proceso, llamada a fork(), distincion entre padre e hijo: proceso padre: continua ejecutando el codigo despues de fork(), proceso hijo: puede ejecutar un programa diferente utilizando execve(). Luego llamado a execve(), sigue la ejecucion independiente.\n",
    "\n",
    "## **Como se termina un proceso?**\n",
    "\n",
    "La terminacion de un proceso marca el final de su ciclo de vida en el SO. Un proceso puede finalizar su ejecucion de diversas maneras, cada una con sus propias causas y procedimientos. Un proceso puede terminar voluntariamente: puede decidir terminar su ejecucion de manera voluntaria cuando ha completado su tarea o cuando ejecuta una condicion que le impide continuar. Este tipo de terminacion generalmente implica que el proceso devuelva un codigo de retorno al SO, indicando el resultado de su ejecucion. Otra forma de terminar un proceso es la terminacion por parte del SO: El SO puede decidir terminar un proceso por diversas razones, generalmente relacionadas con la seguridad, estabilidad o cumplimiento de politicas del sistema. Una ultima forma de terminar un proceso es por accion de un usuario. Un usuario puede decidir terminar un proceso manualmnete utilizando herramientas proporcionadas por el SO. Este metodo es util para cerrar aplicaciones que no responden o que necesitan ser detenidas por alguna razon.\n",
    "\n",
    "## **Que es el estado de un proceso?**\n",
    "\n",
    "Un proceso atraviesa varios estados a lo largo de su ciclo de vida. Estos estados representan la condicion actual del proceso y determinan que acciones puede realizar el SO sobre el. Los principales estados de un proceso incluyen. Ready, running, blocked/waiting, suspended/stopped, zombie, terminated.\n",
    "\n",
    "## **Para que sirve el scheduling?**\n",
    "\n",
    "El schedling es el responsable de decidir que proceso en estado listo sera ejecutado a continuacion. Existen diferentes algoritmos de planificacion que determinan como se asigna el tiempo de CPU: Roun Robin: timeslice fijo, First-Come, First-Served: los procesos se ejecutan en el orden en que llegaron, Prioridad: los procesos con mayor prioridad se ejecutan antes que los de menos prioridad, Shortes Job Next (SJN): Se ejecutan primero los procesos con la menor duracion estimada.\n",
    "\n",
    "## **Que es un hilo?**\n",
    "\n",
    "Un hilo es la unidad mas pequenia de procesamiento que puede ser garantizada de manera indpendeinte por un SO. Los hilos permiten que un proceso realice multiple tareas concurrentemente dentro de un mismo espacio de memoria, aprovechando mejor los recursos del sistema y mejorando la eficiencia y responsividad de las aplicaciones.\n",
    "\n",
    "## **Cuales son las diferencias entre procesos y hilos?**\n",
    "\n",
    "Cada proceso tiene su propio espacio de direcciones de memoira, aislado de otros procesos, mientras que los hilos de un mismo proceso comparten el mismo espacio de direcciones de memoria. En cuanto a la proteccion, hay una proteccion estricta entre procesos, un proceso no puede acceder directamente a la memoria de otro, por el lado de los threads no hay proteccion entre hilos de un mismo proceso, cualquier hilo puede acceder y modificar la memoria compartida. Cada proceso tiene un unico puntero de ejecucion, mientras que cada hilo tiene su propio puntero de ejecucion y stack, permitiendo multiples flujos de control dentro del mismo proceso. Los procesos son entidades independientes con sus propios recursos, los hilos comparten recursos del proceso padre, como descriptores de archivos y memoria. La comunicacion entre procesos es mas compleja y costosa, mientras que entre hilos es mas sencilla y rapida, ya que comparten el mismo espacio de memoria. La creacion y terminacion de procesos es mas costosa en terminos de recursos y tiempo, la creacion y terminacion de hilos es mas ligera y rapida. \n",
    "\n",
    "## **Que es el context switching?**\n",
    "\n",
    "El context switching es el proceso mediante el caul el SO alterna la ejecucion de un proceso o hilo por otro. Este mecanismo es escenail para permitir la multiprogramacion y la multitarea, ya que permite que multiples procesos o hilos compartan los recursos de la CPU de manera eficiente.\n",
    "\n",
    "## **Como funciona el cambio de contexto?**\n",
    "\n",
    "Cuando la CPU esta ejecutando un proceso o hilo y el sistema operativo decide que debe ceder el control a otro, realiza los siguientes pasos basicos: guardar el contexto actual, seleccionar el proximo proceso o hilo, cargar el nuevo contexto.\n",
    "\n",
    "## **Cual es el impacto del cambio de contexto?**\n",
    "\n",
    "Cada vez que se realiza un context switch, la CPU debe dedicar tiempo y recursos a este proceso, lo que puede afectar el rendimiento general del sistema. Minimizar estos costos es escencial para mejorar la eficiencia y la responsividad del SO.\n",
    "\n",
    "## **Que son las prioridades?**\n",
    "\n",
    "La prioridad en un SO es un valor asignado a cada proceso o hilo que indica su importancia relativa en comparacion con otros procesos. Esta prioridad influye en el orden en que el planificador del SO asigna tiempo de CPU a los procesos y como gestiona los recursos del sistema. Las prioridades pueden ser estaticas (asignadas de manera fija al crear el proceso) o dinamicas (pueden cambiar durante la ejecucion del proceso en funcion de ciertos criterios o eventos).\n",
    "\n",
    "## **Que es el scheduling**\n",
    "\n",
    "El scheduling es el proceso mediante el cual el SO decide que proceso o hilo se ejecutara a continacion en la CPU. Dado que la CPU es un recurso limitado, y multiples procesos y hilos compiten por el, la planificacion eficiente es escencial para asegurar un uso optimo de los recursos del sistema, manteniendo la responsibidad y garantizar que las tareas critics se completen a tiempo.\n",
    "\n",
    "## **Que es priority inversion?**\n",
    "\n",
    "La inversion de prioridad ocurre cuando un proceso de alta prioridad necesita un recurso que esta siendo utilizado por un proceso de baja prioridad, pero no puede acceder al recurso hasta que el proceso de baja prioridad lo libere. Mientras tanto, otros procesos de prioridad media pueden monopolizar la CPU, bloqueando indirectamente al proceso de alta prioridad. La solucion seria que el proceso de baja prioridad herede temporalmente la prioridad del prorceso de alta prioridad, lo que permite que el proceso de baja prioridad termine mas rapido y libere el recurso.\n",
    "\n",
    "## **Que es el collaborative multiprocessing/multithreading?**\n",
    "\n",
    "El multiprocesamiento/multihilo colaborativo es un enfoque donde los proceso o hilos ceden voluntariamente el control de la CPU para permitir que otros procesos o hilos sean ejecutados. En este modelo, la responsabilidad de gestionar el tiempo de CPU recae en las propias tareas, las cuales deben colaborar para compartir equitativamente los recursos del sistema. El funcionamiento es el siguiente: ceder el control, cada proceso o hilo debe ceder explicitamente el control de la CPU en puntos especificos de su ejecucion. Planficacion basada en cooperacion: El SO asume que los procesos colaboran y que ninguno monopoliza la CPU. Gestion manual de timeslices: Los hilos deben gestionar manualmente cuanto tiempo ejecutan antes de cender el control, lo que puede llevar a desequilibrios si no se implementa correctamente.\n",
    "\n",
    "## **Que es el preemptive multiprocessing/multithreading?**\n",
    "\n",
    "El multiprocesamiento/multihilo preventivo es un enfoque donde el SO controla de manera activa la asignacion de tiempo de CPU a los procesos y hilos. El SO puede interrumpir una tarea en ejecucion para asignar tiempo a otra, basandose en prioridades, politicas de planificacion y la necesidad de manetener la equidad y la eficiencia del sistema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "## **Que sucede si tengo multiples procesos que quieren utilizar un recurso?**\n",
    "\n",
    "Hay que serializar los procesos. \n",
    "\n",
    "## **Que puedo hacer si tengo un solo procesador para serializar los procesos?**\n",
    "\n",
    "Una idea seria utilizar una variable flag y levantar su valor con un registro RAX, una vez que obtenemos el valor realizamos un test y saltamos si el valor es 1 porque esto indicaria que el recurso esta ocupado, si por el contrario, la flag es 0 entonces empiezo a utilizar el recurso y incremento el RAX. En algun momento terminare de utilizar el recurso, borrare el registro RAX y grabare el 0 en el flag. Si tenemos una sola tarea, no habria problema con este codigo, el problema aparece cuando tenemos dos tareas o threads ejecutando el codigo anterior, el proceso se fijara si el flag esta en 0 para saber si esta libre, si no esta en 0 se altera y si esta en 0 lo pone en 1, lo usa y luego lo pone en 0 devuelta. El problema es que si viene un task switch habra un uso simultaneo de recursos. \n",
    "\n",
    "## **Que es un task switch?**\n",
    "\n",
    "Un task switch o cambio de tarea es el proceso mediante el cual el SO pausa la ejecucion de una tarea o hilo y transifere el control a otra. Es fundamental apra la multitarea, permitiendo que multiples tareas compartan el tiempo de CPU de manera eficiente.\n",
    "\n",
    "## **Que es una operacion atomica?**\n",
    "\n",
    "Una opearcion atomica es aquella que se ejecuta indivisiblemente, sin posibilidades de interrupciones que puedan causar inconsistencias en entornos concurrentes. En arquitecturas x86, la instruccion XCHG es un ejemplo de operacion atomica.\n",
    "\n",
    "## **Cual es el funcionamiento de la instruccion XCHG?**\n",
    "\n",
    "La instruccion XCHG intercambia el contenido de dos operandos. Por ejemplo, XCHG AX, flag intercambiara el valor del registro AX con el de flag. Esta operaacion se realiza de manera atomica, lo que significa que no puede ser interrumpida. Es util para imlementar mecanismos de sincronizacion como locks. Esta operacion permite verificar y establecer el estado del flag en una sola instruccion, evitando condiciones de carrera en entornos de un solo nucleo. Este tipo de instrucciones que se ejecutan en un solo paso se llaman Read-Modify-Write.\n",
    "\n",
    "## **Que sucede con si tenemos multiples nucleos?**\n",
    "\n",
    "En sistemas con multiples nucleos, cada nucelo puede ejecutar hilos de manera concurrente. Aunque las operaciones atomicas como XCHG funcionan bien en entornos de un solo nucleo, surgen desafios cuando multiples nucleos intentan ejecutar estas operaciones simultaenamente. Esto sucede porque aunque XCHG es atomica a nivel de un solo nucleo, no proporciona sincronizacion entre multiples nucleos. Cada nucleo puede tener su propia cache de la memoria y sin una coherencia de cache adecuada las operaciones simultaneas pueden pasar por alto las actualizaciones realizadas por otros nucleos.\n",
    "\n",
    "## **Cual puede ser una solucion si tenemos multiples nucleos?**\n",
    "\n",
    "Utilizar un prefijo lock. En arqitecturas x86, el prefijo LOCK puede usarse con ciertas instrucciones (como XCHG) para asegurarse que la operacion se realice de manera exclusiva en todos los nucleos. Al aplicar LOCK, se bloquea el bus de memoria, evitando que otros nucleos accedan o modifiquen la misma direccion de memoria simultaneamente. Estas acciones que se activan al aplicar LOCK son a nivel de hardware.\n",
    "\n",
    "## **Que es un spinlock?**\n",
    "\n",
    "Un spinlock es un tipo de mecanismo de sincronizacion que el SO nos ofrece, que se utiliza para controlar el acceso exclusivo a un recurso compartido en entornos concurrentes. A diferencia de otros mecanismos de bloqueo que ponen a los hilos en espera, los spinlocks mantienen al hulo en un bucle de espera activo hasta que el bloque este disponible.\n",
    "\n",
    "## **Que es un semaforo?**\n",
    "\n",
    "Un semaforo es un mecanismo de sincronizacion utilizado para controlar el acceso de multiples procesos o hilos a recursos compartidos de manera coordinada. Los semaforos ayudan a prevenir condiciones de carrera y garantizan que los recursos se utilicen de manera eficiente y segura. Un semaforo mantiene un contador interno que representa el numero de permisos disponibles. Tiene dos operaciones basicas: Wait (P) cuya finalidad es decrementar el contador. Si el contador es mayor que cero, el hilo puede continuar, de lo contrario, el hilo se bloquea hasta que un permiso este disponible. Por otro lado Signal (V) incrementa el contador y, si hay hilos bloqueados esperando, uno de ellos se desbloquea. Hay dos tipos de semaforos, binarios y tipo contaodr. Los bianrios son miliares a un mutex, solo pueden tomar los valores 0 o 1, mientras que el semaforo contador puede tomar multiples valores positivos, permitiendo controlar multiples accesos simultaneos a un recurso.\n",
    "\n",
    "## **Que es un Mutex?**\n",
    "\n",
    "Un mutex es un mecanismo de sincronizacion utilizado para controlar el acceso exclusivo a un recurso compartido entre multiples hilos (threads) dentro de un mismo proceso. Su proposito principal es asegurar que solo un hilo pueda ejecutar una seccion critica de codigo a la vez, evitando condiciones de carrera y garantizando la integridad de los datos compartidos.\n",
    "\n",
    "## **Que es un pipe?**\n",
    "\n",
    "Un pipe es un mecanismo de comunicacion entre procesos que permite que un proceso envie datos a otro proceso de manera unidireccional. Escencilamnete, actua como una tuberia fisica a traves de la cual los datos fluyen desde un extremo (escritor) al otro extremo (lector) en el orden en que se envian. Cuando se crea un pipe se crean dos descriptores de archivos, desriptor de escritura y descriptor de lectura.\n",
    "\n",
    "## **Como es el proceso de comunicacion a traves de un pipe?**\n",
    "\n",
    "El proceso escritor abre el extremo de escritura del pipe, envia datos escribiendo en el pipe y almacena esos datos en un bufer interno del SO. Por el lado del proceso lector, este abre el extremo de lectura del pipe, lee los datos del pipe que fluyen desde el extremo de escritura y los datos se leen en el orden en que se escribieron. Despues de escribir los datos, el proceso escritor cierra el extremo de escritura para indicar que no enviara mas datos y el proceso lector al cerrar el extremo de lectura, finaliza la comunicacion.\n",
    "\n",
    "## **Que es una senial**\n",
    "\n",
    "Las seniales son mecanismos de comunicacion asincronica utilizados por el sistema operativo para notificar a los procesos sobre eventos especificos. Funciona como interrupciones que pueden ser enviadas por el SO o por otros procesos, permitiendo que un proceso reaccione a ciertas condiciones o eventos sin necesidad de una supervision constante.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "## **Explica el problema de reubicacion de ejecutables**\n",
    "\n",
    "Cuando un SO inicia un nuevo proceso, debe cargar el programa correspondiente desde el almacenamiento secundario (como un disco duro) a la memoria principal (RAM). Este proceso implcia varias etapas critica: Carga del programa, asignacion de memoria, ubicacion en memoria, modificacion del codigo y transferencia de control. La reubicacion es el proceso de ajsutar las direcciones de memoria dentro del programa para que pueda ejecutarse correctamente desde una posicion diferente a la orgininalmnete compilada.\n",
    "\n",
    "## **Que es la reubicacion estatica?**\n",
    "\n",
    "La reubicacion estatica implica modificar el ejecutable en tiempo de carga.\n",
    "\n",
    "## **Que es la reubicacion dinamica?**\n",
    "\n",
    "Para superar las limitaciones de la reubicacion estatica, se desarrollan tecnicas de reubicacion dinamica que incluyen: Direcciones relativas o base-offset: el codigo utiliza direcciones realtivas que se ajustan en tiempo de ejecucion mediante registros especailes (como el registro base). Memoria virtual, segmentado y paginado.\n",
    "\n",
    "## **Describi las soluciones y tecnicas modernas para mitigar los problemas asociados con la reubicacion estatica de ejecutables**\n",
    "\n",
    "Una de las tecnicas para solucionar el tema de reubicacion estatica es la utilizacion de memoria virtual, la memoria virtual abstrae la memoria fisica, permitiendo que cada proceso tenga su propio espacio de direcciones. Esto facilita la reubicacion dinamica y mejora la seguridad y la estabilidad. Otras tecnicas son las paginacion y la segmentacion. La paginacion divide la memoria en bloques de tamanio fijo (paginas), permitiendo una asignacion mas flexible y reduciendo la fragmentacion. La segmentaicon por otro lado permite dividir el programa en segmentos logicos (como codigo, datos, pila), facilitando la proteccion y la gestion eficiente de la memoria.\n",
    "\n",
    "## **Explica el problema de proteccion entre tareas**\n",
    "\n",
    "En un sistema multiproceso, multiples tareas o procesos se ejecutan concurrentemente, compartiendo los recursos del sistema, incluida la memoria principal (RAM). Sin una adecuada proteccion de memoria, un proceso podria acceder o modificar la memoria de otro proceso, lo que podria llevar a corrpucion de datos, fallos en el sistema o vulnerabilidades de seguridad. Tambien podria interferir con el funcionamiento del SO, afectando la estabilidad global. Es crucual implementar mecanismos que ailsen cada proceso y controlen el acceso a memoria, asegurando que cada tarea opere dentro de su propio espacio de direcciones sin interferir con otras.\n",
    "\n",
    "## **Cual es la limitacion de la reubicacion estatica en cuanto a la proteccion de memoria?**\n",
    "\n",
    "La reubicacion estatica resuelve el problema de cargar ejecutables en diferentes ubicaciones de memoria, pero no aborda la proteccion entre tareas. Con la reubicacion estatica cada proceso puede cargarse en cualquier lugar de la memoria, ajustando sus direcciones relativa. Sin embargo, todos los procesos comparten el mismo espacio de direcciones fisico, lo que significa que, teoricamente, un proceso podria acceder a cualquier parte de la memoria si no hay restricciones adicionales.\n",
    "\n",
    "## **Cuales son los mecanismos de proteccion de memoria?**\n",
    "\n",
    "Para implementar la proteccion de memoria, los SOs modernos utilizan una combinacion de soporte de hardware y politicas de software. Algunos mecanismos son: Unidad de gestion de memoria MMU, espacios de dirrecciones virtuales, tablas de paginas y tablas de segmentos, mecanismos de proteccion a nivel de SO.\n",
    "\n",
    "## **Explica una solucion al tema de seguridad**\n",
    "\n",
    "Una solucion al tema de seguridad es el segmentado. La segmentacion es una estrategia de gestion de memoria que divide la memoria de un proceso en segmentos logicos de diferetne tamanio, como codigos, datos, pila, etc. Cada segmento se maneja de manera independiente, lo que facilita la organizacion, proteccion y reubicacion de la memoria. Los objetivos de la segmentacion son los siguientes: reubiccion dinamica: permite cargar un programa en cualquier unicacion de la memoria sin necesidad de modificar el codigo del programa, proteccion de memoria: asegura que cada proceso solo acceda a sus propios segmentos de memoria, evitando accesos no autoirzados a otros procesos o al SO, modularidad y organizacion: facilita la gestion de diferentes partes de un programa de manera independiente.\n",
    "\n",
    "## **Como es el proceso de acceso a memoria con segmentacion?**\n",
    "\n",
    "Primero se genera una direccion logica, el programa genera una direccion logica compuesta por un segmento y un desplazamiento (offset). Luego se realiza una traduccioa a la direccion fisica, para ello se utiliza un sumador que al desplazamiento le suma la direccion base del segmento obtenida del registro de segmentacion correspondiente y un comparador que verifica que el desplazamiento no exceda el limite del segmento. Si la verificacion es exitosa, la direccion fisica resultante se utiliza para acceder a la memoria. De lo contrario, se genera una excepcion de proteccion.\n",
    "\n",
    "## **Como afecta la reubicacion y la proteccion la segmentacion?**\n",
    "\n",
    "En cuanto a la reubicacion, al tener un registro de segmentacion que actua como base, el sistema puede cargar el mismo programa en diferentes ubicaciones de memoria sin necesidad de modificar el codigo. Solo se ajusta elr egistro de segmentacion correspondiente. Por el lado de la proteccion, el comparador de limites garantiza que un proceso no pueda acceder fuera de sus segmentos asignados, protegiendo asi la mempria de otros procesos y del SO.\n",
    "\n",
    "## **Que es el memory swapping?**\n",
    "\n",
    "El memory swapping es una tecnica de gestion de memoria utilizada por los SOs para manejar situaciones en las que la memoria fisica (RAM) disponible no es suficiente para alojar todos los procesos activos. Cuando la demanda de memoria excede la capacidad, el SO intercambia partes de la memoria de un proceso activa hacia un espacio de almacenamiento secundario (generalmente disco duro), liberando asi memoria para otros procesos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Como hace el SO para detrminar donde se ubicara en memoria una tarea recien cargada?**\n",
    "\n",
    "Para determinar donde ubicara la tarea en la memoria fisica, es necesario establecer una base y un limite. La base es la direccion inicial donde se cargraa el segmento de memoria del proceso y el limite es el tamanio del segmento asignado al proceso. Ademas de leer el ejecutable y conocer los segmentos que contiene, el SO debe llevar un registro detallado de que areas de la memoria estan ocupadas y cuales estan libres para evitar conflictos y optimizar el uso de recursos.\n",
    "\n",
    "## **Explica cuales son los dos metodos principales para mantener un registro del uso de memoria**\n",
    "\n",
    "Hay dos enfoques para mantener un registro del uso de memoria, bitmaps y listas.\n",
    "\n",
    "## **Cuales son los algoritmos utilizados si se utilizan listas para gestionar la memoria?**\n",
    "\n",
    "Firs fit (primer ajuste): recorre la lista de bloques desde el inicio y asigna el primer blouqe que sea lo suficientmente grande para el proceso. Next fit: Similar al first fit pero comienza la busqeuda desde donde se dejo la ultima asignacion, formando una lista circular. Best fit recorre toda la lista de bloques libres y asigna el blouqe que deja el menor desperdicio posible. Worst fit asigna el bloque mas grande disponible que sea suficiente para el proceso. Quick fit: mantiene multiples listas de bloques libres categorizados por tamanio, cada lista contiene blouqes de tamanios especificos o rangos de tamanios.\n",
    "\n",
    "## **Que es la memoria virtual?**\n",
    "\n",
    "La memoria virtual es una tecnica que permite a los SOs utilizar mas memoria de la que fisicamente esta disponible en el hardware. Esto se logra mediante la combinacion de la memoria fisica (RAM) con el almacenamiento en disco creando una ilusion de un espacio de direcciones continuo y amplio para cada proceso.\n",
    "\n",
    "## **Cuales son los probleams que surgen por la memoria fisica limitada**\n",
    "\n",
    "En sistemas con multiples procesos, la suma total de memoria requerida por todos los segmentos de los procesos puede exceder la memoria fiscia disponble. Esto puede llevar a situaciones donde no hay suficiente memoria para asignar nuevos procesos o para expandir los segmentos existentes de los procesos en ejecucion.\n",
    "\n",
    "## **Como se maneja el tema de la memoria fisica limitada?**\n",
    "\n",
    "Para manejar este problema, los SOs implementan el memory swapping que consiste en mover aprtes de la memoira que no esta en uso activo hacia el disco duro, liberando asi espacio en la memoria fisica para otros procesos o para expandir segmetnos que lo requieran.\n",
    "\n",
    "## **Cuales son las limitaciones del sistema de segmentacion?**\n",
    "\n",
    "En un sistema de segmentacion puro, cada segmento de un proceso debe residir en una parte contigua de memoria fisica. Si un segmento ncesita crecer mas alla del espacio disponible en la memoria fisica, el sistema no puede proporcionarle la memoria adicional, lo que limita la flexibilidad y la escalabilidad de los procesos.\n",
    "\n",
    "## **Cual es una solucion al tema de la segmentacion?**\n",
    "\n",
    "Para superar estas limitaciones, se introduce el manejo de memoria virtual, que ofrece una abstraccion mas flexible y eficiente. En este sistema, cada proceso percibe un espacio de direcciones virtuales continuo y amplio, que puede ser mucho mayor que la memoria fisica disponible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "# **Entrada y salida**\n",
    "\n",
    "## **Como funcionaban los primeros teclados?**\n",
    "\n",
    "Los primeros teclados arrancaron como tablas con teclas conectadas en una placa base.\n",
    "\n",
    "## **Que son los teclados de membrana?**\n",
    "\n",
    "Los primeros teclados eran bastante costosos por lo que se tuvo que migrar a alternativas mas economicas, los teclados de membrana ya no tienen mas teclas sino que tienen dos contactos dibujados en el impreso y hay una tecla arriba de una lamina de goma que tiene una parte conductora, cuando esa tecla se aprieta, se unen los conductores y se genera una descarga electrica. Este tipo de teclado permitio bajar drasticamente los costos.\n",
    "\n",
    "## **Como funcionan los teclados?**\n",
    "\n",
    "Los teclados en su hardware tienen un chip controlador que que observa que teclas hay apretadas y le reporta al preocesador esa informacion con una interrupcion. En todas las maquinas actuales, cuando se hace el contacto en las teclas, el procesador recibe un codigo de make, y cuando se suelta el contacto, el procesador recibe un codigo de breake.\n",
    "\n",
    "## **Como funcionaban los primeros ratones?**\n",
    "\n",
    "Inicialmente eran una bolita que movian dos rodillos y giraban un potenciometro, a medida que iba girando cambiaba el valor de la resistencia. Tenian sus problemas porque esas resistencias se terminaban gastando. Luego se terminaron cambiando los potenciometros por encoders.\n",
    "\n",
    "## **Como funcionan los ratones hoy en dia?**\n",
    "\n",
    "Hoy en dia los ratones tienen un led en la parte inferior y una pequenia camara que saca fotos unas 1000 veces por segundo. Un pequenio procesador dentro del raton analiza el movimiento entre cuadros de las imperfecciones sobre las que se desplaza el raton y detemrina la direccion del movimiento, esa informacion es transferida a la CPU. El funcionamiento de los ratones en distintas superficies depende de la longitud de onda con la que trabaje el raton. \n",
    "\n",
    "## **Explica las pantallas tactiles del tipo resistivas**\n",
    "\n",
    "Las pantallas tactiles resistivas consisitan de dos membranas transparentes, la superior era flexible y tenia pequenios hilos conductores. Cuando uno presionaba la membrana superior se doblaba internamente y hacia que uno de los conductores se tocara con el otro. Esto aumento drasticamente la resolucion de las pantallas tactiles, pero existia el problema de tocar en mas de un lugar al mismo tiempo.\n",
    "\n",
    "## **Como estan hechas las pantallas tactiles de hoy en dia?**\n",
    "\n",
    "Hoy en dia se elimino el tema de usar las membranas plasticas, en cambio estan hechas por capas de vidrio con pocos atomos de espesor. Este tipo de pantallas no entran en la categoria de resistivos por culpa de la utilizacion del vidrio, ya que este no cede. Entonces lo que se hace es medir un fenomeno denominado capacidad que se mide al acercar el dedo a la pantalla. \n",
    "\n",
    "## **Como eran los primeros monitores?**\n",
    "\n",
    "Los primeros monitores eran tubos de rayos catodicos, tenian un emisor de electrones que se aceleraban hacia adelamnte formando un rayo y cuando esos pegaban contra el frente eran devueltos del otro lado. \n",
    "\n",
    "## **Como funcionaban los monitores de cristal liquido**\n",
    "\n",
    "Tienen un liquido especial metido entre dos vidrios con un polizador. El problema es que el tiempo de alinear y desalinear las moleculas no se puede controlar con exactitud.\n",
    "\n",
    "## **Explica los monitores LED**\n",
    "\n",
    "Cada punto tien tres leds, azul, verde y rojo y graduando la intensidad de estos colores se puede representar cualquier otro color. Los leds tardan mucho menos en prenderse y apagarse.\n",
    "\n",
    "## **¿Cómo funciona una impresora láser? Describa el proceso contínuo que permite depositar el tinte sobre el papel.**\n",
    "\n",
    "El proceso limpieza, carga, dibujo, deposito, transferencia y fusion es el proceso contiguo que se da en las impresoras laser. Primero, se limpia el rodillo, luego pasa por un sistema de carga que carga negativamente el cilindro, acto seguido, hay un laser que pasa de lado a lado del cilindro emitiendo pulsos de luz, en los lugares donde pulsa la luz, calienta el material y hace que la carga que esa contenida se disipe, el material se hace conductor con la luz y la carga se va. Osea que al pasar por el laser quedan mucho puntos algunos con carga electrica y otros sin carga, acto seguido viene para el toner, el toner es un material con particulas muy chiquitas que fueron cargadas positivamente dentro del cartucho, cuando pasa cerca el rodillo, en lo lugares donde tiene carga negativa, el toner no se adhiere. Al ser expuesto al toner, la parte el rodillo con carga negativa atrae particulas de toner, que son transferidas al papel con carga positiva. Por ultimo un fusor calienta el polvo para convertirlo en una capa fija al papel. El proceso es continuo y simultaneo.\n",
    "\n",
    "## **Como funciona una impresora de chorro de tinta?**\n",
    "\n",
    "Escencialmente tienen un cabezal que tiene una fila de agujeros que van a dibujar los puntos, estos puntos se dibujan fila por fila y luego se pasa al de abajo. Hay un pedazo de conductor con una resistencia, se coloca un dispositivo que tiene tinta con un orificio muy chico en la parte de abajo. Ese orificio es tan chico que la tinta no cae, la forma de hacer que la tinta salga es calentar muy rapidamente la resistencia, la cual en consecuencia generara una burbuja de vapor en la tinta que se expandera muy rapidamente y generara que salga un chorro de tinta hacia el papel.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "# **BUSES**\n",
    "\n",
    "## **Defina el concepto de BUS. Indique en qué difieren los paralelos de los serie**\n",
    "\n",
    "El bus conecta dos bloques que tienen que intercambiar informacion. Son lineas electricas que tienen una funcion a fin, que es implementar la comunicacion entre bloques. Si nos basamos en la arquitectura, particularmente en el grado de paralelismo, podemos dividir en dos grandes grupos. Los paralelos: en donde la informacion viaja por varias lineas en forma simultanea, por ejemplo el buss de adress y el bus de datos. Por otro lado hay unos que son en serie, en donde hay una linea de datos y la informacion se serializa mandando 1 bit a la vez uno atras del otro. Si pongo buses que son paralelos, al tener muchas lineas juntas que trabajan en paralelo el ancho de banda va a ser mayor pero el costo del hardware va a ser mayor tambien, por ende hay que tener en cuenta el trade off entre velocidad y costo de hardware.\n",
    "\n",
    "## **Que parametros fisicos limitan la velocidad a la que se pueden transmitir los datos?**\n",
    "\n",
    "Dado que la transmision de los datos depende de un cable fisico, hay parametros o variables fisicas que afectaran la velocidad de esa transmision de los datos. Algunos de los parametros son los siguientes: El tamanio del bus: el tiempo que tarda la luz en barrer 35 cm (que es lo que puede llegar a medir un bus) es de alrededor de unos nanosegundos, si comparamos eso con el tiempo de vasculacion de una compuerta logica vemos que da menos que nanosegundos. Entonces habra un cuello de botella entre el tiempo que tarda la electricidad en viajar por el bus y el tiempo de vasculacion de las compuertas logicas que son las que procesan esa electricidad para generar y almacenar informacion. La tension es otro factor importante, principalmente el nivel de voltaje que se pone en los buses. Hay que tener cuidado con ese nivel de voltaje porque los buses especialemnte los que estan fuera del gabinete son propensos a agarrar ruido electrico, entonces si tenemos voltajes pequenios ese ruido electrico sera muy significativo y puede llevar a confusiones. El problema es que tampoco se puede aumentar mucho la tension para mitigar el tema del ruido electrico ya que mucha tension implicara mayor consumo de energia. Otra cosa que afecta la velocidad es la utilizacion de multiples lineas para mandar un solo bit de informacion. Una solucion inteligente para mitigar el ruido electrico es utilizar informacion diferencial, en donde tengo dos cables por los que mando informacion y en vez de mirar la tension en ambos cables miro la diferencia de potencial entre los cables, lo que dara como resultado que el ruido se cancele bajando los potenciales y el consumo.\n",
    "\n",
    "## **Que tipos de BUSES hay?**\n",
    "\n",
    "Hay buses diferenciales y buses que no son diferenciales. Normalmente los que estan dentro del mother como el bus de adress y el bus de datos no suelen ser diferenciales, para equipos que van conectados afuera, como el bus USB o HDMI que estan mas propensos a agarrar ruidos esos si son diferenciales.  \n",
    "\n",
    "## **Explica el BUS ISA**\n",
    "\n",
    "Ya no se usa extrictamente pero servia para conectar distintas placas a las computadoras. Estos buses estan muy claramente documentados y cada diseniador podria hacer lo que queria, soportaba transferencias de 8 y 16 bits a la vez y habia 8 mega transferencias por segundo con 2 waits states. La velocidad de transferencia era de 5mbps.\n",
    "\n",
    "## **Explica el BUS PCI**\n",
    "\n",
    "Es el susesor de los BUSes ISA. La diferencia con el ISA es que subieron los bits a 32. Esta mayor cantidad de bits implica que las lineas esten mas pegadas por ende requiere mayor tecnologia. Se baja la tension con la idea de bajar el consumo, permitiendo llevarlo de 8 mega transferencias por segundo a 33 mega transferencias por segundo. El bus PSI subde 5mbps a 133mbps\n",
    "\n",
    "## **Explica el BUS PCI Express**\n",
    "\n",
    "Hoy en dia las placas de cualquier PC tienen conectores PCI express. Hay una familia de conectores chicos y grandes, se hace esta separacion entre chicos y grandes por la doble presion impuesta por el hardware, por un lado las placas de video que cada vez quieren mas bits de adress, por otro lado estan los dispositivos que requieren pocos bits de adress. Entonces frente a esta presion se crearon diferentes alterantivas para cumplir con los requerimientos de todos. Para mayor velocidad y para poder lidiar con el ruido ahora todos los canales son diferenciales. Hay distintos tipos de PCI dependendiendo de sus links, estos links determinan la velocidad de transferencia, va desde un link (1gbyte por segundo) a 16 links. Ahora estamos en 126.000 mbps. \n",
    "\n",
    "## **Que es el Hot Swapping?**\n",
    "\n",
    "Hoy en dia algunos buses soportan la conexion de los dispositivos en caliente y otros no. Esta diferencia es tanto de software como de hardware. Por el lado del hardware se tiene que preveer que el usuario pueda enchufar distintas cosas, y por otro lado hay que modificar el software, si agrego un dispositivo hay que cargar un driver para que pueda ser utilizado. Entonces para que los buses soporten conexion en caliente necesitamos hardware apropiado (que no se queme cuando conectamo cosas con energia1 y por otro lado necesitamos software que sea capaz de detectar esos cambios.)\n",
    "\n",
    "## **Explica el BUS SATA**\n",
    "\n",
    "El BUS SATA es el primer bus que soporta hot swapping. Es el actual bus que se usa para los discos rigidos. Tienen una tasa de 600mbps y los cables no son mas largos que los dos 2m. Se utilizan hoy en dia tanto para notebooks como para pcs de escritorio. Lo mas importante es que soporta hot swapping, osea que soporta desenchufar el disco en caliente, esto es muy importante cuando se habla de RAIDS.\n",
    "\n",
    "## **Explica el USB**\n",
    "\n",
    "Nace como un concepto distinto a lo que eran los buses de transferencia masiva para discos. El principal objetivo de estos buses es que sea versatil y permita la conexion de multiples cosas, y ademas, que sea barato, y ademas con algo innovador que era el plug and play. El USB tiene que cuando se conecta un dispositivo el SO reconoce que se conecto y carga el driver automaticamente. Entonces el USB lo que hace es que cuando se conecta, envia alguna informaciond de quien es o que es y a traves de una base de datos se carga el driver correspondiente. El USB 3.0 tiene del orden de 600mbps de velocidad de transferencia maxima. El vinculo entre el bus y el ordenador es el dispositivo llamado root hub. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "# **GPUs**\n",
    "\n",
    "## **Cuales son las diferencias entre la CPU y la GPU?**\n",
    "\n",
    "La CPU es de proposito general, es decir tienen muchas instrucciones que sirven para cualquier proposito, es decir los nucleos de la CPU no estan pensados para algun proposito en particular. Por otro lado la GPU, esta pensada para especializarse en un conjunto muy chico de cosas que saben hacer muy rapidamente. Tanto la CPU como la GPU ambos presentan la arquitectura de Von Neumann, ambos con una unidad de proceso, un procesador, RAM, y algunos dispositivos de entrada y salida. En el caso de la GPU los dispositivos de I/O son mas limitados que en la CPU porque no tienen que comunicarle mucho al usuario ademas de la salida del monitor, si tienen I/O con la CPU para lograr la comunicacion entre ellos. Las GPUs tienen muchos mas cores que la CPUs, esto es principalmente porque como las operaciones que tienen que hacer son simples y en gran cantidad, es factible tener muchos cores ya que seran bastante simples a comparacion de los cores de CPU. Las GPUs tienen memorias mas rapidas que las CPUs, eso no es una ventaja en si misma porque la CPU tienen mas memoria que la GPU, entonces las GPUs tienen menos memoria y por ende suele ser mas rapida. Las CPUs estan muy documentadas, las GPUs como normalmente se inventaron para graficar y los drivers los suele hacer la propia compania, no estan tan documentadas como la CPU. Las CPUs estan pensadas para una interfaz directa con el usuario mientras que la GPU no.\n",
    "\n",
    "## **Cuales son los protocolos mas comunes que expone el SO y que implementa a traves de drivers?**\n",
    "\n",
    "La GPU no tiene interfaz con el usuario por ende la unica forma que se comunique con la CPU es con un conjunto de direcciones dentro del espacio de memoria del procesador principal. El procesador principal le envia comandos a la GPU para que esta ejecute. No es muy necesario saber como se intercambia la informacion entre la GPU y la CPU. El problema aparece cuando se empeiza a utilizar la GPU para algo distinto de lo que fue originalmente creado, es decir si se usa para algo que no sea graficar. El SO expone una serie de protocolos que presentan interfaces de alto nivel para que las aplicaciones puedan dibujar. Los protocolos mas comunes son: OpenGL (descontinuado), Vulkan (mantenido), Direct3D (Microsoft), CUDA (NVIDIA), ZLUDA (AMD).\n",
    "\n",
    "## **Como es el funcionamiento de uan GPU?**\n",
    "\n",
    "Las GPUs estan pensadas para especializarse en un conjunto muy chico de tareas pero que saben realizar de manera muy rapida. Escencialmente una GPU calcula tanto numeros de punto flotantes, como proyecciones de un objeto desde distintos puntos de vista del observador, sombras del objeto, etc. Es decir, realizan calculos matematicas sencillos y en gran cantidad para representar un objeto en su totalidad en el espacio. Su funcionamiento consiste en lo siguiente: la forma mas rapida de definir objetos en el espacio es dividir su superficie en poligonos, generalmente por simplicidad en traingulos. Los triangulos tienen la particularidad que permiten la aplicacion de mucha matematica sobre ellos dado que el error de los triangulos para representar superficies es batante bajo. Hay distintas formas de definir estos triangulos. Una forma es definirlos como triangulos independientes, a estos triangulos debo pasarle cada vertice por separado, es decir 3 vertices a cada triangulo. De todas formas no es la manera mas optima de construir triangulos ya que cuando represento objetos a traves de triangulos siempre los trianuglos van a estar tocandose entonces hay vertices que se definiran solos. Entonces, otra forma que es mejor para dibujar los triangulos en vez de tratarlos como independientes es utilizar tiras de triangulos en donde el primer vertice se especifica y luego se generan los otros dos vertices en base al especificado. Entonces siempre ire agregando un solo vertice para armar los triangulos. La ultima alternativa es utilizar un abanico de triangulos. La GPU, cuando grafica lo que hace es utilizar una combinacion de estos metodos para graficar los triangulos que representaran los objetos. Por ultimo, La GPU no solamente permite tomar el objeto como si fuese un monton de lineas en el espacio y las proyecta sino que tambien puede darle un color en cada extremo y luego la GPU encontrara todas las texturas intermedias. Entonces el funcionmaiento es: definicion del cuerpo ----> agreado de texturas y colores.\n",
    "\n",
    "## **¿Qué tipos de cores tiene una GPU? ¿Para qué se utiliza cada tipo?**\n",
    "\n",
    "Las GPUs hoy en dia tienen 3 tipos de cores. Los que se llaman nucleos CUDA que estan destinados a generar texturas en los poliedros. Uno define un poliedro y esos cores estan especificamente pensados para detectar y armar la textura entre cada uno de esos triangulos. Otros nucleos son los tensores, que son los que hacen las rototraslaciones, no solo determinan como se ve sino que tambien si se ve o no. Por ultimo hay otro tipo de nucleo que se llaman RT o ray traaacing que escencialmente estan orientados especificamente a calcular la interseccion entre una linea y un objeto, es decir donde esa linea intersecta con un determinado objeto. En circustancias normales, es decir para lo que tradicionalmente se crearon las GPUs todos estos nucleos trabajan con un pipelining, primero hay que usar los nucelos tensores para rotrotrasladar los triangulos y ver cuales y como va a ver el punto de vista del observador. Despues, habra que utilizar los nucleos de CUDA para calcular la textura, el brillo, reflejo de cada uno de esos triangulos, y finalmente opcionalmnete, si hay disparos o algo se utilizar los nucelos RT. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "## **Que desencadeno el uso de las maquinas virtuales?**\n",
    "\n",
    "Anteriormente en la historia era comun que las empresas tuvieran un servidor por cada aplicativo importante que tuvieran. Cada proyecto requeria comprar un sistema nuevo. El problema de los data centers es que son caros en cuanto a lugar, alquiler de racs tambien era caro por todo lo que involucraba.\n",
    "\n",
    "## **Cual era la idea principal de las maquinas virtuales?**\n",
    "\n",
    "La idea es que en vez de que el SO le virtualice a una aplicacion los dispositivos, otro programa, que se llama virtualizador le virtualiza el hardware al SO. Hay que virtualizar la RAM, los procesadores, teclados, mouse, placa de video. Cada SO va a tener un hardware abajo pensando que es suyo cuando en realidad lo tiene compartido con otros. Este programa que permite crear diferentes instancias de hardware en cada una de las cuales podre instalar un SO se lo denomina hipervisor. La estructura es de la siguiente manera: La virtualizacion del hardware hace que cada uno de los SO vea su propio hardware, ahora tenemos la parte fisica y en vez de tener el SO sobre la parte fisica como teniamos antes con sus drivers que manejaban el hardware y a su vez el SO con sus tareas, tenemos ahora sobre el fisico el hipervisor que expone n isntancias del hardware llamadas maquinas virtuales y en cada maquina virtual voy a instalar un SO y cada uno de esos SO correran un conjunto de tareas. De la misma manera que el SO veia sus propios recursos y protegia a las tareas entre si, el hipervisor hace lo mismo pero entre SOs. El hiperivosr es el que mantiene el roden para que no haya conflicto en el hardware. Esto da un aprovechamiento mucho mas eficiente de los recursos. Cuando un SO no tiene carga o nada que hacer le da control a los otros, y en el mismo hardware puedo correr distintos tipos de aplicaciones.\n",
    "\n",
    "## **Que hardware se tuvo que implementar en las maquina virtuales y porque?**\n",
    "\n",
    "Cada SO piensa que el hardware es suyo y de uso exclusivo. De la misma manera que antes teniamos un task switch cuando habia que scarle el procesador a una tarea para darselo a otra, lo mismo va a tener que hacer el hipervisor pero entre SOs. \n",
    "\n",
    "## **Que es el over assignment**\n",
    "\n",
    "El proceso de dar mas recursos que los disponbiles se llama over-assignment. Para algunas cosas esto funciona pero no para la memoria, un SO cuando no esta usando el disco no hace nada, no lo toca, entonces el virtualizador se da cuenta que no se quiere utilizar, lo mismo cuando una tarea no imprime nada en pantalla, no escribe nada, ahora con la memoria es un problema aparte. Los SOs utilizan toda la memoria, la tengna usada o no. A medida que las tareas piden memoira, el SO se las da, lo que sobra se guarda como cache. Cuando uno carga una VM le carga un driver llamado ballon driver. La idea de ese driver es que el virtualizador necesita memoria y tiene que ver a que SO le puede sacar, ahi aparece el driver, este driver se fija si hay ram libre o si esta siendo utilizada por tareas, en el caso de encontrar un SO que este utilziando memoria como cache entonces ese SO es candidato a que se le saque memoria. Para hacer esto, el ballon driver le pide al SO un bloque de memoria, borra cosas de cache y apenas el SO recibe la memoria la libera dejando al SO con un crocker libre. De esa forma el virtualizador puede lograr que disitnos SO huespedes bajen la cache. El over-assignment bien usado optimiza los recursos y permite tener la misma performance que con servidores individuales a un menor costo.\n",
    "\n",
    "## **Que es Docker?**\n",
    "\n",
    "Docker permite generar un contenedor y distribuir en una unica imagen todo junto, codigo, configuracion, etc. No es necesariamente muy optimo ni que se use en un ambiente de data centers para correr aplicaciones pero es algo que muchas veces se usa cuando hay que mandarle algo a un usuario que no es de alto nivel. La imagen contiene todo, inclusive el SO. Una aplicacion distribuida con Docker puede ejecutarse en cualquier SO soportado sin cambios. \n",
    "\n",
    "## **Como se configura una VM?**\n",
    "\n",
    "Primero se declara que SO le voy a poner y cuanta memoria voy a darle en funcion de la memoria real. El orden de booteo se carga en el bios y hay dos formas de arranque, la forma de boot sector o el EFI. Luego se configura el reloj con un chip de tiempo real que tienen los SOs. Luego se pueden asginar los cores que uno quiere a cada VM. En cuanto a la pantalla, cuanta memoria de video, monitores y controlador grafico (chip que va a emular el video). En cuanto al almacenamiento, las maquinas tienen dos alternativas: uno es que sea en un archivo del sitema de archivos del hipervisor. La otra alternativa es crear un archivo que contiene todo, un volumen del storage. Luego tenemos el audio, para emularlo el controlador emula un audio. Luego adapatadores de red, que permiten pedir el el chip que queres emular, aparte de emular el chip, cuando una maquina virtual quiere sacar algo por la red, probablemnete el hipervisor tenga varias placas de red fisicas y yo quiera que algunas vms hablen por una y otras por otra. En cuanto al USB, uno a veces quiere enchufar dos dispositivos y quiero que uno lo vea una maquina y otro otra, entonces hay un menu que permite asignar un USB a una maquina determinada. En definitiva, el objetivo de la configuracion de la VM es seleccionar el hardware, que hardawre se le expondra al SO. \n",
    "\n",
    "## **Que es una granja de maquinas virtuales?**\n",
    "\n",
    "Hay hipervisores que permiten trabajar en conjunto en granjas. Pones varios host fisicos, en todos instalamos la misma marca de hipervisor y con eso pongo todas las VMS. Los hipervisores se pueden hablar entre si.\n",
    "\n",
    "## **Que ventaja nos da que los hipervisores que corren en distintos hosts fisicos se puedan habar entre si?**\n",
    "\n",
    "Si un hipervisor por ejemplo detecta que esta con mucha carga de CPU, que esta con muchas tareas corriendo a la vez, puede tomar algunas acciones, la mas importante es que puede mudar la maquina en tiempo real a que se siga ejecugtando en otro hipervisor. Esto no solamente nos da inmunidad ante fallas sino que tambien seguimos mejorando el aprovechamiento de los recursos, unas de las ventajas que teniamos de poner varias maquinas en un mismo servidor fisico era que como no todas las maquinas virtuales utilizan todos los recursos a la vez, el hipervisor podia balancear y aprovehcar mejor el host fisico. Ahora podemos hacer un aprovechamiento entre servidores.\n",
    "\n",
    "## **Como se mudan las maquinas virtuales a otro servidor fisico en una granja de maquinas virtuales?**\n",
    "\n",
    "Las granjas de maquinas virtuales mudan la maquina completa a otro servidor fisico migrando una \"foto\" de la memoria de la VM y los datos quie el hipervisor tiene de esa VM (solo algunos GB). A parte de la foto de la RAM con los registros, la aplicacion puede tener conexiones de red abierta y sobreotodo puede tener conexiones con los discos, entonces ese si seria un problema, estaria el problema de acceder a los archivos prebios. Entonces cuando se trabaja con esta arquitectura no se tienen los discos fisicamente en un servidor, en definitiva un disco es una conexion de red. Esta posibilidad de migrar maquinas en caliente es lo que se llama Machine-Migration y se produce de forma transparente al SO. ya que tenemos esta posibilidad de pasar las maquinas mientras se ejecutan de un lado a otro, esto podemos no solo usarlo por un tema de balance de CPU, si se llega a detectar un pronlema siempre el hipervisor tiene la psoibilidad de mover eso a otra maquina fisic y scara las maquinas del servidor que tiene problemas. El servidor con problemas puede repararse sin que los usuarios noten el problema. Esta funcionalidad por este motivo tambien es conocida como HA (High Availability). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-family: 'Source Code Pro'; font-size: 24px;\">\n",
    "\n",
    "## **Como se puede aumentar la capacidad de computo de una CPU?**\n",
    "\n",
    "Hay muchas tecnicas para aumentar la capacidad de computo de una CPU pero siempre va a haber un limite. A lo largo del tiempo, los megahertz de los procesadores fueron aumentando del orden de 10x cada decada. Hasta que se estabilizo en los gigahertz en los 2000s hasta la actualidad. El problema es un limite fisico, no se pueden achicar mas los transistores, entonces para crecer por encima del limite no queda otra que agregar mas nucleos. El problema aca es como dividir las tareas. Para tareas con poca interaaccion, utilizar n cpus resultara en n veces mas velocidad, pero para sub-tareas con mucha interaccion mutua, se debe compartir memoria y el proceso se hace menos optimo y mas complicado.\n",
    "\n",
    "## **Como estan divididos los sistemas multiprocesador?**\n",
    "\n",
    "Los sistemas multiprocesador estan divididos en los de memoria compartida o shared-memory y los de shared interconect.\n",
    "\n",
    "## **Que son los sistemas multiprocesador de shared memory (memoria compartida)?**\n",
    "\n",
    "En los sistemas de memoria compartida, todos acceden a la memoria en las mismas condiciones, nadie tiene prioridad o privilegio para acceder a la memoria. Hay un tema importante que es el sincronismo entre las CPUs. En la medida que se logre la separacion entonces tener memoria compartida es un beneficio. \n",
    "\n",
    "## **Que son los sistemas multiprocesador de shared interconnect?**\n",
    "\n",
    "En estos sistemas tenemos dos tipos de memoria, cada CPU tiene su propia memoria en donde trabaja, ejecuta los programas y calcula los resultados, pero se dispone de una segunda memoria de interconexion compartida en la cual se hace el intercambio de los datos de una CPU y otra.\n",
    "\n",
    "## **En que difieren estos dos tipos de sistemas?**\n",
    "\n",
    "Los de memoria compartida son mas dificiles de construir en terminos de hardware porque hayq ue resolver el problema de la contencion por el recurso pero la realidad es que son faciles de programar porque cada programa asume que tiene todo. Los de shared interconnect son mas dificiles de construir porque no solamente hayq eu resolver la interaccion entre CPU y memoria sino que tengo que resolver la interconeccion de esas posiciones de memoria, el programador tiene que saber que hay abajo para poder dividir su problema en distintas tareas de roam que tenga un conjunto finito de datos de entrada, de resultados y como van a interactuar los procesos.\n",
    "\n",
    "## **En que categorias puede divirse un sistema multiprocesador con memoria compartida?**\n",
    "\n",
    "Un sistema de memoria compartida puede divirse en dos categorias. UMA y NUMA. En las UMA, el acceso es uniforme, todos tienen el mismo privilegio en todas las zonas de memoria, es decir no solamnete ven todo sino que ven todo en igualdad de condiciones. Las NUMA por el contrario ven todo pero es como que hay preferencia o mas velocidad en un sector.\n",
    "\n",
    "## **Describi las UMA**\n",
    "\n",
    "En el caso en que todas las CPUs ven la totalidad de memoria, uno de los casos son las UMA. En UMA, tenemos tres arquitecturas que se usan. Las primeras son las que tenemos en las notebooks, que son las de single bus, estas tienen un solo bus de direcciones y un solo bus de datos y cada core cuando quiere acceder le habla a un MMU y esa unidad de manejo de memoria coordina los accesos. Son mas faciles de hacer pero obviamente es valido y funcional para no mas de 10 o 20 CPUs porque llega un momento en que el bus comienza a ser un cuello de botella porque por ese bus pasan todas las transacciones de todas las CPUs, si empiezo a poner muchos cores empiezo a tener un cuello de botella y pierdo performance. Para evitar eso es que se colocan caches entre cada uno de los CPUs y la memoria para evitar que todos los accesos de la CPU terminan en acceso a memoria porque sino claramente el cuello de botella aparece antes. Si no tengo ningun cache y cada acceso a la CPU tiene que ir a memoria entonces claramente todos tienen que ir a memoria y el cuello de botella ya esta ahi. Hay que tener un protocolo de coherencia de cache. Hasta aca teniamos un unico bus y n CPUs, que pasa si hacemos una matriz, entonces una CPU podria acceder por distintos caminos a cualquiera de las memorias, la memoria esta partida en pedazos pero esto reduce mucho la cantidad de choques, esto es a expensas de mucho hardware, estamos poniendo caminos redundantes para todas las memorias, es mas facil porque elimine los caches entonces no hay contenciones, no hay que andar usando un protocolo de coherencia de cache, pero obviamente esto crece con el cuadrado de pares memoria-CPU. Una solucion de compromiso es la red omega y escencialmente lo que haces es que cada uno de los nodos toma dos entradas y genera dos salidas, de esa forma, si bien es verdad que cada CPU no tiene un camino propip a la memoira, es una opcion de compromiso que se utiliza hoy en dia en las supercomputadoras, la cantidad de nodos necesarios para la interconexion en vez de crecer con el cuadrado de pares memoria-CPU crece con el logaritmo, entonces obviamente la cantidad de hardware involucrado es mas chico. La mayor parte de la programacion de estos sistemas involucra conocer por abajo el sistema para saber como distribuyo las estructuras en las distintas memorias, etc. \n",
    "\n",
    "## **Describi las NUMA**\n",
    "\n",
    "en las NUMA, ahora cada una de las CPUs tiene una memoria de uso no exclusivo pero de mas rapido acceso que los demas. Lo que es importante para que se mantenga la categoria de memoria compartida es que todas las CPUs vena la totalidad del espacio de memoria, en las UMA cualquier posicion tarda lo mismo en las NUMA no. En estos casos se puede o no usar el cache, dependera de la arquitectura en particular y sobretodo de cual es la diferencia de velocidad entre la memoria local y la memoria remota.\n",
    "\n",
    "## **Describi los sistemas multiprocesamiento de shared interconnect**\n",
    "\n",
    "Este es el caso en el que las CPUs no tienen el espacio de memoria compartido en su totalidad, solamente tienen alguna zona de memoria comun donde se intercambian datos. Para que esto funcione tenemos que tener dos condiciones, primero una forma rapida de intercambiar los datos entre los distintos bloques. Por otro lado, es imprescindible que las aplicaciones conozcan el hardware. \n",
    "\n",
    "## **Que es un sistema distribuido?**\n",
    "\n",
    "Estos sistemas estan directamente separados geograficamente unos del otro. No solamente se usan para ganar en performance sino que tambien para ganar en confiabilidad, es poco probable que si hacemos el mismo calculo en dos maquinas en paralelo que una se rompa y no obtengamos el resultado. Estos sistemas distribuidos son como una tercera categoria, no comparten ninguna parte de la memoira, tienen sistemas independientes que trabajan por separado y a lo sumo se intercambian algun bloque de informacion. Los nodos son muy especificos, realizan trabajos especificos. Esto no solamente se utiliza para aumentar la cantidad de recursos sino que tambien son muy utiizadas en aplicaciones de mucha confiabilidad. Tenemos multiples sistemas haciendo lo mismo y los resultados seran comparados por un tercer sistema para asegurarse que nada difiera.\n",
    "\n",
    "## **Cuales son las clasificaciones dentro de los sistemas distribuidos?**\n",
    "\n",
    "Una de las clasificaciones es cliente servidor, en donde tengo distintos servidores que calculan distinta cosas y tengo clientes que en funcion de que necesitan tiene que saber a quien esta pregunando, aca la informacion es casi unidireccional. Otros, directamente son totalmente anarquicos, cualquiera puede requerir informacion a cualquiera y se estan pasando informacion entre ellos permanentemente. Otra alternativa, intermedia en este caso es el middleware, en donde hay un pquenio SO que se encarga de compartir los datos. Despues hay algunos que dividen en mas capas, no solamente la capa de SO y de tarea sino que divide las capas por ejemplo en interfaaz con el usuario, procesamiento y datos. Esto es muy comun en la industria, por ejemplo un banco."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
