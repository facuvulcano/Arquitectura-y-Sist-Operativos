{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejoras de performance\n",
    "\n",
    "Cuando vamos a intentar cualquier mejora de performance dentro de cualquier procesador automaticamente aparecen compromisos, no es que puedo mejorar todo. Las variables mas importantes de este compromiso son las siguientes: la velocidad, el precio de ese componente, la confiabilidad y el consumo de energia. Son las cosas mas importantes a tener en cuenta. No podemos maximizar todo, normalmnete tener mas velocidad implica tener hardware especifico para hacer ciertas cosas, eso obliga a que el chip sea mas complejo ocupe mas lugar y sea mas caro, al ser mas complejo implica mas componentes y mas componentes implican mas desconfiabilidad. Tambien al tener mas componentes probablemente consuma mas energia. Normalmente la unica variable que uno puede hacer que mejora todo es cuando uno mejora la tecnologia. A una dada tecnologia mejorar una implica perder un poco en otra. Un tema importante con el precio es que el precio aumenta mas que linealmnete con la complejidad, eso hace que al aumentar la complejidad el precio se dispara. El motivo es porque los chips se hacen en obleas de un determinado tamanio y en esa oblea se hacen cierta cantidad de chips, pero tambien por fallas de la tecnologia aparecen una cierta cantidad de impurezas que arruinan ciertos lugares, no es lo mismo en una oblea tener 2 impurezas de 1000 chips que dos impurezas de 5 chips, eso hace que el precio aumente mas que linealmente.\n",
    "\n",
    "## Alternativas para lidiar con este compromiso\n",
    "\n",
    "La primer alternativa podria ser bajar la cantidad de ciclos por instruccion. Si pudiera hacer que las instrucciones se ejecuten en menos ciclos aumentaria la velocidad. Otra alternativa podria ser hacer circuitos mas optimos, en un procesador cada paso se llevaba a cabo con un ciclo de clock, sincronizado con ese ciclo. Ahora porque no podiamos subir indefinidamente ese ciclo de clock? porque en realidad en un ciclo de clock ibamos a poner los datos que qeuriamos que se ejecuten y en el proximo ciclo tomariamos los resultados, entonces a medida que bajo el ciclo podria ser que quiera tomar los resultados antes que esten. Una tercera alternativa seria solapar los pasos de dos instrucciones o mas a la vez.\n",
    "\n",
    "## Reduccion de los ciclos por instruccion\n",
    "\n",
    "Por ejemplo en nuestro procesador de ejemplo cada instrucción llevaba 4 ciclos de reloj, pero en el último no se hacía nada. Se podría modificar la unidad de  control para que cada una lleve 3. Es un poco más complejo usar un contador módulo 3 que uno módulo 4, pero es realizable. Para que sea simple, todas las operaciones llevaban 4 ciclos de reloj, pero había  instrucciones como CLR podrían usar 2 (buscar instrucción y borrar registro). Una vez más, que las instrucciones lleven distinta cantidad de ciclos complejiza la unidad de control, pero también es realizable. Esto lleva a que haya que hacer disenios mas novedosos como hardware intermedio. Otra alternativa que puede bajar los ciclos por instruccion es no tener instrucciones siempre del mismo tamanio, si tengo instrucciones de distinto tamanio podria ser que a las instrucciones con menos parametros le asigne codigos de operacion mas chicos y menos bytes por instruccion para que se ejecuten mas rapido.\n",
    "\n",
    "## Bajar la complejidad de los circuitos\n",
    "\n",
    "Para reducir los tiempos, hay que usar diseños más óptimos, que utilicen menor \n",
    "cantidad de transistores y/o transistores más pequeños y por ende más rápidos\n",
    "\n",
    "Hay dos temas mas que involucran la velocidad del clock. Una es, no solamente esta el tiempo de una compuerta en encontrar la salida correcta sino que tambien hay un tiempo cuando hay dispositivos que estan fisicamente lejanos, esto puede ser 20cm en una maquina. La realidad es que estamos hablando en velocidades tan altas que el tiempo que tarda la electricidad de un chip que esta alejado al procesador hay que tenerla en cuenta. En electronica se dice que achicar es acelerar. Achicar en general es acelerar salvo ahora que se esta estancando, los transitores hoy en dia trabajan a nivel atomico.\n",
    "\n",
    "El otro tema que limita la frecuencia del reloj es el consumo de energia, este esta relacionado con la cantidad de transiciones. La disipacion de potencia esta relacionada con la cantidad de bits que cambian de ceros a uno, cuando achico tengo mas transiciones por ende mas disipacion de potencia. En la medida que los procesadores tienen mucha disipacion de potencia esto calienta mucho. Hoy en dia los procesadores son chatos y grandes y un disipador importante para sacar ese calor. Entonces tengo limitaciones pero achicar implica hacerlo mas rapido\n",
    "\n",
    "## Solapar mas de una instruccion\n",
    "\n",
    "Ejecutar mas de una instruccion a la vez. A este proceso de solapamiento de instrucciones se lo llama pipelining. Simulatneamente se van a ejecutar varias instrucciones, en la medida que pongo mas instrucciones en el medio logro un mayor aprovechamiento pero aumenta la probabilidad de coliciones, es decir aumenta la probabilidad de que dos instrucciones quieran utilizar el mismo recurso. El stalling es cuando hay que detener una instruccion porque no tengo el recurso necesario para ejecutar la instruccion. Este proceso de ejecucion simultanea de instrucciones es mas complejo de lo que parece, pero tenemos algunos probleams mas, si la primer instruccion es un salto por ej y la condicion se cumple, la instruccion siguiente nunca deberia haberse ejecutado, entonces arrancamos a ejecutar algo que enrealidad no habia que ejecutar. Otro problema posible es que pasa si la primera instruccion guarda los datos en un registro, y la segunda instruccion usa ese registro para hacer alguna segunda cuenta, la realidad es que no podemos empezar a ejecutar la segunda ni bien largamos la primera porque la segunda depende del resultado que me de la primera. Entonces todo esto agrega complejidad. Estas cosas se hacen a pesar de que sean complejas porque hay una competencia feroz por la performance. Todas estas complejidades hay que solventarlas pero se hacen. Hoy en dia todos los procesadores utilizan esta tecnologia.\n",
    "\n",
    "## Otra forma: La memoria cache.\n",
    "\n",
    "El acceso a la memoria era una parte determinante del tiempo total de ejecucion. A lo largo de la historia por temas tecnologicos las memorias han crecido menos en su velocidad que lo que han crecido los procesadores. Hoy cualquier sistema moderno se veria obligado a tener varios gigas de RAM y esa RAM podria ser implementado como RAM estatica o dinamica, si la poniamos como RAM dinamica era lenta, si la poniamos como RAM estatica teniamos que tapizar de chips de RAM las cosas pero esto traia dos problemas, por un lado es caro y por otro lado tengo que poner mas lejos por ende lo hace mas lento. Entonces las dos cuestiones hacen que el acceso a la RAM sea un problema. Entonces para solventar este problema una alternativa fue utilizar memorias RAM rapidas, caras chicas y rapidas y la idea de usar un cache en definitiva es interponer esa RAM rapida entre el procesador y la RAM grande de gran capacidad lenta. Si tengo que ir a buscar algo y no esta en el cache entonces tengo que ir a buscarlo a RAM, obviamente el tiempo va a ser el mismo que no haber usado el cache. Pero si el dato esta en el cache, entonces podria tener una ganancia importante. Obviamente hay que definir una politica para ver que voy a guardar en el cache. La idea es que cuando hay un dato que es util cuando lo traigo de RAM no solamente se lo lleva el procesador y lo usa sino que tambien queda copiado en el cache. Entonces si el procesador lo vuelve a usar no tiene que ir a buscarlo a la memoria RAM que es lenta y esta fisicamente lejos. El cache esta cerca del procesador, de hecho hoy en dia viene adentro del procesador. Esta politica no es trivial, de alguna forma hay que adivinar si el dato se va a usar o no. Normalmente por como son los programas el codigo puede tener saltos ciclos y conviene guardar las cosas que estan relativamente cerca del instruction pointer. Los datos no necesariamente. En definitiva los procesadores modernos separan en dos politicas. En algunos casos, hasta justifica utilizar un segundo cache, muy chiquito y muy rapido muy cerquita del procesador, uno intermedio mas grande pero rapido cerca del procesador y la RAM. Este cache chiquito se conoce nivel 2 o L2. Escencialmente hay dos grandes formas de implementar caches, los primeros que se conocen como DIRECT MAP, cada posicion de memoria de la gran RAM que tenemos puede estar solamente en un lugar del cache, si esta esta en un lugar, si no esta en ese lugar entonces no esta. Dado una dirección, la parte baja determina la fila dentro del caché donde podría estar, luego el bloque alto se compara con el bloque alto que se guardó en la fila y se define si está o no. Si está la fila contiene el dato. La otra alternativa son los n-way aca puedo guardar en muchos lugares o inclusive en todos, seria mucho mas versatil desde el punto de vista del cache pero mas complicado desde el punto de vista de implementacion. Esto no solamente se usa para leer, sino que tambien para escribir. Hay dos politicas basicas de escritura, una es el write through que es escribir a traves, es decir, cuando el procesador quiere escribir le pega la memoria y si le corresponde tambien le pega el cache. Escribe en paralelo en la memoria final y si corresponde en el cache tambien. La otra alternativa se llama write back, es cuando el procesador escribe el dato en el cache y me voy a hacer otra cosa y el cache tiene logica para bajarlo a memoria, es mas eficiente porque hay un nuevo bloque que se llama manejador de memoria que se encarga de sacar todo lo que esta modificado en el cache pero no en la memoria y ir actualizando la memoria. \n",
    "\n",
    "## Prediccion del salto\n",
    "\n",
    "Otro tema es la prediccion del salto. Si yo supiese que la instruccion que viene va a tener un salto condicional a mi me convendria si se que no va a saltar seguir ejecutando las instrucciones de arriba, pero si se que va a saltar ejecutar las instruccion a las que fue a parar el salto. Claramente eso es imposible, tendria que conocer el futuro. En el pasado habia prefijos que se le colocaban a las instrucciones que le decian a la unidad de control esto probablemente salte o al reves, la unidad de control podia optar por que instrucciones ejecutar, pero esto es una tentativa porque no puedo guardar los resultados hasta saber que salte o no, esto me hara perder tiempom y se llama static branch hit. Esto se dejo de usar, los procesadores modernos analizan un codigo y con un buffer especial guardan para cada uno de los saltos que tienen en el rango estadisticamente cuantas veces salto y cuantas no, utilizan su propia estadistica para saber si va a saltar o no, esto lo hace el procesador con hardware de adentro. Esto tiene la desventaja de que las primeras veces se equivoca mas pero encuadra en el caso concreto de su programa con sus datos. Ésto se llama “Dynamic branch hint”.\n",
    "\n",
    "Porque tengo que ejecutar las instrucciones en orden? si no dependen entre ellos entonces no habria problema en ejecutar instrucciones en desorden. Este es otro tema que permite mejorar la performance. Esta meora se llama izado y permite mover para arriba ciertas instrucciones bajo ciertos casos.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
